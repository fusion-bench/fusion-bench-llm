model,task_category,task_name,metric_name,score
google/gemma-2-2b-it,math,gsm8k_cot,"exact_match,flexible-extract",0.5056861258529188
google/gemma-2-2b-it,multilingual,m_mmlu_fr,"acc,none",0.2645328851882973
google/gemma-2-2b-it,multilingual,arc_fr,"acc_norm,none",0.3772455089820359
google/gemma-2-2b-it,multilingual,hellaswag_fr,"acc_norm,none",0.502784322124652
google/gemma-2-2b-it,multilingual,m_mmlu_es,"acc,none",0.22836358182090896
google/gemma-2-2b-it,multilingual,arc_es,"acc_norm,none",0.3598290598290598
google/gemma-2-2b-it,multilingual,hellaswag_es,"acc_norm,none",0.5142948581181993
google/gemma-2-2b-it,multilingual,m_mmlu_de,"acc,none",0.23336853220696938
google/gemma-2-2b-it,multilingual,arc_de,"acc_norm,none",0.34046193327630453
google/gemma-2-2b-it,multilingual,hellaswag_de,"acc_norm,none",0.45975661827497866
google/gemma-2-2b-it,multilingual,m_mmlu_ru,"acc,none",0.22995310217575152
google/gemma-2-2b-it,multilingual,arc_ru,"acc_norm,none",0.330196749358426
google/gemma-2-2b-it,multilingual,hellaswag_ru,"acc_norm,none",0.4659188955996549
google/gemma-2-2b-it,instruction_following,ifeval,"inst_level_loose_acc,none",0.6330935251798561
google/gemma-2-2b-it,coding,mbpp_plus,"pass_at_1,none",0.0
google/gemma-2-2b-it,coding,humaneval_plus,"pass@1,create_test",0.0
google/gemma-2-2b-it,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5570242420426904
google/gemma-2-2b-it,safety,toxigen,"acc_norm,none",0.6234042553191489
google/gemma-2-2b-it,safety,winogender,"winogender_all:acc,none",0.5583333333333333
MergeBench/gemma-2-2b-it_instruction,math,gsm8k_cot,"exact_match,flexible-extract",0.4746019711902957
MergeBench/gemma-2-2b-it_instruction,multilingual,m_mmlu_fr,"acc,none",0.3388587579252922
MergeBench/gemma-2-2b-it_instruction,multilingual,arc_fr,"acc_norm,none",0.3515825491873396
MergeBench/gemma-2-2b-it_instruction,multilingual,hellaswag_fr,"acc_norm,none",0.5037481259370314
MergeBench/gemma-2-2b-it_instruction,multilingual,m_mmlu_es,"acc,none",0.31138443077846106
MergeBench/gemma-2-2b-it_instruction,multilingual,arc_es,"acc_norm,none",0.35128205128205126
MergeBench/gemma-2-2b-it_instruction,multilingual,hellaswag_es,"acc_norm,none",0.5196287604011095
MergeBench/gemma-2-2b-it_instruction,multilingual,m_mmlu_de,"acc,none",0.33330819128073613
MergeBench/gemma-2-2b-it_instruction,multilingual,arc_de,"acc_norm,none",0.34901625320787
MergeBench/gemma-2-2b-it_instruction,multilingual,hellaswag_de,"acc_norm,none",0.4637062339880444
MergeBench/gemma-2-2b-it_instruction,multilingual,m_mmlu_ru,"acc,none",0.2811563004536019
MergeBench/gemma-2-2b-it_instruction,multilingual,arc_ru,"acc_norm,none",0.33704020530367834
MergeBench/gemma-2-2b-it_instruction,multilingual,hellaswag_ru,"acc_norm,none",0.4755176876617774
MergeBench/gemma-2-2b-it_instruction,instruction_following,ifeval,"inst_level_loose_acc,none",0.579136690647482
MergeBench/gemma-2-2b-it_instruction,coding,mbpp_plus,"pass_at_1,none",0.0
MergeBench/gemma-2-2b-it_instruction,coding,humaneval_plus,"pass@1,create_test",0.0
MergeBench/gemma-2-2b-it_instruction,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5581347443520455
MergeBench/gemma-2-2b-it_instruction,safety,toxigen,"acc_norm,none",0.6510638297872341
MergeBench/gemma-2-2b-it_instruction,safety,winogender,"winogender_all:acc,none",0.5444444444444444
MergeBench/gemma-2-2b-it_math,math,gsm8k_cot,"exact_match,flexible-extract",0.5633055344958302
MergeBench/gemma-2-2b-it_math,multilingual,m_mmlu_fr,"acc,none",0.2591093117408907
MergeBench/gemma-2-2b-it_math,multilingual,arc_fr,"acc_norm,none",0.358426005132592
MergeBench/gemma-2-2b-it_math,multilingual,hellaswag_fr,"acc_norm,none",0.524951809809381
MergeBench/gemma-2-2b-it_math,multilingual,m_mmlu_es,"acc,none",0.22896355182240888
MergeBench/gemma-2-2b-it_math,multilingual,arc_es,"acc_norm,none",0.36153846153846153
MergeBench/gemma-2-2b-it_math,multilingual,hellaswag_es,"acc_norm,none",0.5333902282910177
MergeBench/gemma-2-2b-it_math,multilingual,m_mmlu_de,"acc,none",0.23291597526022023
MergeBench/gemma-2-2b-it_math,multilingual,arc_de,"acc_norm,none",0.35072711719418304
MergeBench/gemma-2-2b-it_math,multilingual,hellaswag_de,"acc_norm,none",0.4709649871904355
MergeBench/gemma-2-2b-it_math,multilingual,m_mmlu_ru,"acc,none",0.230260628892135
MergeBench/gemma-2-2b-it_math,multilingual,arc_ru,"acc_norm,none",0.32677502138579984
MergeBench/gemma-2-2b-it_math,multilingual,hellaswag_ru,"acc_norm,none",0.48004745470232957
MergeBench/gemma-2-2b-it_math,instruction_following,ifeval,"inst_level_loose_acc,none",0.6139088729016786
MergeBench/gemma-2-2b-it_math,coding,mbpp_plus,"pass_at_1,none",0.0
MergeBench/gemma-2-2b-it_math,coding,humaneval_plus,"pass@1,create_test",0.0
MergeBench/gemma-2-2b-it_math,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5492512276911342
MergeBench/gemma-2-2b-it_math,safety,toxigen,"acc_norm,none",0.5659574468085107
MergeBench/gemma-2-2b-it_math,safety,winogender,"winogender_all:acc,none",0.5305555555555556
MergeBench/gemma-2-2b-it_coding,math,gsm8k_cot,"exact_match,flexible-extract",0.45109931766489764
MergeBench/gemma-2-2b-it_coding,multilingual,m_mmlu_fr,"acc,none",0.43831640058055155
MergeBench/gemma-2-2b-it_coding,multilingual,arc_fr,"acc_norm,none",0.3857998289136014
MergeBench/gemma-2-2b-it_coding,multilingual,hellaswag_fr,"acc_norm,none",0.5436924394945385
MergeBench/gemma-2-2b-it_coding,multilingual,m_mmlu_es,"acc,none",0.43932803359832007
MergeBench/gemma-2-2b-it_coding,multilingual,arc_es,"acc_norm,none",0.38290598290598293
MergeBench/gemma-2-2b-it_coding,multilingual,hellaswag_es,"acc_norm,none",0.5620866225730745
MergeBench/gemma-2-2b-it_coding,multilingual,m_mmlu_de,"acc,none",0.41280736159300047
MergeBench/gemma-2-2b-it_coding,multilingual,arc_de,"acc_norm,none",0.3678357570573139
MergeBench/gemma-2-2b-it_coding,multilingual,hellaswag_de,"acc_norm,none",0.4924210076857387
MergeBench/gemma-2-2b-it_coding,multilingual,m_mmlu_ru,"acc,none",0.3668793726454986
MergeBench/gemma-2-2b-it_coding,multilingual,arc_ru,"acc_norm,none",0.3438836612489307
MergeBench/gemma-2-2b-it_coding,multilingual,hellaswag_ru,"acc_norm,none",0.4960094909404659
MergeBench/gemma-2-2b-it_coding,instruction_following,ifeval,"inst_level_loose_acc,none",0.5971223021582733
MergeBench/gemma-2-2b-it_coding,coding,mbpp_plus,"pass_at_1,none",0.3915343915343915
MergeBench/gemma-2-2b-it_coding,coding,humaneval_plus,"pass@1,create_test",0.0
MergeBench/gemma-2-2b-it_coding,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5356133460600826
MergeBench/gemma-2-2b-it_coding,safety,toxigen,"acc_norm,none",0.5553191489361702
MergeBench/gemma-2-2b-it_coding,safety,winogender,"winogender_all:acc,none",0.5694444444444444
MergeBench/gemma-2-2b-it_multilingual,math,gsm8k_cot,"exact_match,flexible-extract",0.38362395754359363
MergeBench/gemma-2-2b-it_multilingual,multilingual,m_mmlu_fr,"acc,none",0.3377893209074937
MergeBench/gemma-2-2b-it_multilingual,multilingual,arc_fr,"acc_norm,none",0.3575705731394354
MergeBench/gemma-2-2b-it_multilingual,multilingual,hellaswag_fr,"acc_norm,none",0.5486185478689227
MergeBench/gemma-2-2b-it_multilingual,multilingual,m_mmlu_es,"acc,none",0.3106344682765862
MergeBench/gemma-2-2b-it_multilingual,multilingual,arc_es,"acc_norm,none",0.37606837606837606
MergeBench/gemma-2-2b-it_multilingual,multilingual,hellaswag_es,"acc_norm,none",0.5652869639428205
MergeBench/gemma-2-2b-it_multilingual,multilingual,m_mmlu_de,"acc,none",0.34997737215266256
MergeBench/gemma-2-2b-it_multilingual,multilingual,arc_de,"acc_norm,none",0.3567151411462789
MergeBench/gemma-2-2b-it_multilingual,multilingual,hellaswag_de,"acc_norm,none",0.5037361229718189
MergeBench/gemma-2-2b-it_multilingual,multilingual,m_mmlu_ru,"acc,none",0.30314446067502115
MergeBench/gemma-2-2b-it_multilingual,multilingual,arc_ru,"acc_norm,none",0.3550042771599658
MergeBench/gemma-2-2b-it_multilingual,multilingual,hellaswag_ru,"acc_norm,none",0.5035591026747196
MergeBench/gemma-2-2b-it_multilingual,instruction_following,ifeval,"inst_level_loose_acc,none",0.5107913669064749
MergeBench/gemma-2-2b-it_multilingual,coding,mbpp_plus,"pass_at_1,none",0.3306878306878307
MergeBench/gemma-2-2b-it_multilingual,coding,humaneval_plus,"pass@1,create_test",0.1402439024390244
MergeBench/gemma-2-2b-it_multilingual,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5198394007592193
MergeBench/gemma-2-2b-it_multilingual,safety,toxigen,"acc_norm,none",0.7712765957446809
MergeBench/gemma-2-2b-it_multilingual,safety,winogender,"winogender_all:acc,none",0.5791666666666667
MergeBench/gemma-2-2b-it_safety,math,gsm8k_cot,"exact_match,flexible-extract",0.4473085670962851
MergeBench/gemma-2-2b-it_safety,multilingual,m_mmlu_fr,"acc,none",0.2838591398670843
MergeBench/gemma-2-2b-it_safety,multilingual,arc_fr,"acc_norm,none",0.3575705731394354
MergeBench/gemma-2-2b-it_safety,multilingual,hellaswag_fr,"acc_norm,none",0.5152066823730992
MergeBench/gemma-2-2b-it_safety,multilingual,m_mmlu_es,"acc,none",0.23488825558722065
MergeBench/gemma-2-2b-it_safety,multilingual,arc_es,"acc_norm,none",0.3487179487179487
MergeBench/gemma-2-2b-it_safety,multilingual,hellaswag_es,"acc_norm,none",0.5276296138254747
MergeBench/gemma-2-2b-it_safety,multilingual,m_mmlu_de,"acc,none",0.27613516367476243
MergeBench/gemma-2-2b-it_safety,multilingual,arc_de,"acc_norm,none",0.3498716852010265
MergeBench/gemma-2-2b-it_safety,multilingual,hellaswag_de,"acc_norm,none",0.46925704526046114
MergeBench/gemma-2-2b-it_safety,multilingual,m_mmlu_ru,"acc,none",0.24017836549550242
MergeBench/gemma-2-2b-it_safety,multilingual,arc_ru,"acc_norm,none",0.32335329341317365
MergeBench/gemma-2-2b-it_safety,multilingual,hellaswag_ru,"acc_norm,none",0.47314495254529765
MergeBench/gemma-2-2b-it_safety,instruction_following,ifeval,"inst_level_loose_acc,none",0.48081534772182255
MergeBench/gemma-2-2b-it_safety,coding,mbpp_plus,"pass_at_1,none",0.0
MergeBench/gemma-2-2b-it_safety,coding,humaneval_plus,"pass@1,create_test",0.0
MergeBench/gemma-2-2b-it_safety,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5119180809817007
MergeBench/gemma-2-2b-it_safety,safety,toxigen,"acc_norm,none",0.5627659574468085
MergeBench/gemma-2-2b-it_safety,safety,winogender,"winogender_all:acc,none",0.5541666666666667

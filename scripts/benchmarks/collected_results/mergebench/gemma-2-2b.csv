model,task_category,task_name,metric_name,score
google/gemma-2-2b,math,gsm8k_cot,"exact_match,flexible-extract",0.28430629264594387
google/gemma-2-2b,multilingual,m_mmlu_fr,"acc,none",0.37598350011458254
google/gemma-2-2b,multilingual,arc_fr,"acc_norm,none",0.40461933276304535
google/gemma-2-2b,multilingual,hellaswag_fr,"acc_norm,none",0.591775540801028
google/gemma-2-2b,multilingual,m_mmlu_es,"acc,none",0.37813109344532775
google/gemma-2-2b,multilingual,arc_es,"acc_norm,none",0.4025641025641026
google/gemma-2-2b,multilingual,hellaswag_es,"acc_norm,none",0.6028376360145082
google/gemma-2-2b,multilingual,m_mmlu_de,"acc,none",0.3733594810680344
google/gemma-2-2b,multilingual,arc_de,"acc_norm,none",0.3849443969204448
google/gemma-2-2b,multilingual,hellaswag_de,"acc_norm,none",0.53266438941076
google/gemma-2-2b,multilingual,m_mmlu_ru,"acc,none",0.3511955101099408
google/gemma-2-2b,multilingual,arc_ru,"acc_norm,none",0.360136869118905
google/gemma-2-2b,multilingual,hellaswag_ru,"acc_norm,none",0.5294434857635893
google/gemma-2-2b,instruction_following,ifeval,"inst_level_loose_acc,none",0.2709832134292566
google/gemma-2-2b,coding,mbpp_plus,"pass_at_1,none",0.42328042328042326
google/gemma-2-2b,coding,humaneval_plus,"pass@1,create_test",0.16463414634146342
google/gemma-2-2b,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.36322745868756484
google/gemma-2-2b,safety,toxigen,"acc_norm,none",0.4319148936170213
google/gemma-2-2b,safety,winogender,"winogender_all:acc,none",0.5708333333333333
MergeBench/gemma-2-2b_instruction,math,gsm8k_cot,"exact_match,flexible-extract",0.3146322971948446
MergeBench/gemma-2-2b_instruction,multilingual,m_mmlu_fr,"acc,none",0.42517760293331297
MergeBench/gemma-2-2b_instruction,multilingual,arc_fr,"acc_norm,none",0.41830624465355004
MergeBench/gemma-2-2b_instruction,multilingual,hellaswag_fr,"acc_norm,none",0.6174769757978154
MergeBench/gemma-2-2b_instruction,multilingual,m_mmlu_es,"acc,none",0.4263536823158842
MergeBench/gemma-2-2b_instruction,multilingual,arc_es,"acc_norm,none",0.4444444444444444
MergeBench/gemma-2-2b_instruction,multilingual,hellaswag_es,"acc_norm,none",0.632067420524856
MergeBench/gemma-2-2b_instruction,multilingual,m_mmlu_de,"acc,none",0.42019912505656964
MergeBench/gemma-2-2b_instruction,multilingual,arc_de,"acc_norm,none",0.41146278870829767
MergeBench/gemma-2-2b_instruction,multilingual,hellaswag_de,"acc_norm,none",0.5592442356959864
MergeBench/gemma-2-2b_instruction,multilingual,m_mmlu_ru,"acc,none",0.3822557084646729
MergeBench/gemma-2-2b_instruction,multilingual,arc_ru,"acc_norm,none",0.3866552609067579
MergeBench/gemma-2-2b_instruction,multilingual,hellaswag_ru,"acc_norm,none",0.5542493528904228
MergeBench/gemma-2-2b_instruction,instruction_following,ifeval,"inst_level_loose_acc,none",0.5191846522781774
MergeBench/gemma-2-2b_instruction,coding,mbpp_plus,"pass_at_1,none",0.4417989417989418
MergeBench/gemma-2-2b_instruction,coding,humaneval_plus,"pass@1,create_test",0.16463414634146342
MergeBench/gemma-2-2b_instruction,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.43904650903868914
MergeBench/gemma-2-2b_instruction,safety,toxigen,"acc_norm,none",0.4319148936170213
MergeBench/gemma-2-2b_instruction,safety,winogender,"winogender_all:acc,none",0.5777777777777777
MergeBench/gemma-2-2b_math,math,gsm8k_cot,"exact_match,flexible-extract",0.5693707354056103
MergeBench/gemma-2-2b_math,multilingual,m_mmlu_fr,"acc,none",0.39294171568253
MergeBench/gemma-2-2b_math,multilingual,arc_fr,"acc_norm,none",0.39863130881094955
MergeBench/gemma-2-2b_math,multilingual,hellaswag_fr,"acc_norm,none",0.6011994002998501
MergeBench/gemma-2-2b_math,multilingual,m_mmlu_es,"acc,none",0.4057297135143243
MergeBench/gemma-2-2b_math,multilingual,arc_es,"acc_norm,none",0.41965811965811967
MergeBench/gemma-2-2b_math,multilingual,hellaswag_es,"acc_norm,none",0.6174525282696821
MergeBench/gemma-2-2b_math,multilingual,m_mmlu_de,"acc,none",0.3945542314074521
MergeBench/gemma-2-2b_math,multilingual,arc_de,"acc_norm,none",0.3849443969204448
MergeBench/gemma-2-2b_math,multilingual,hellaswag_de,"acc_norm,none",0.5418445772843723
MergeBench/gemma-2-2b_math,multilingual,m_mmlu_ru,"acc,none",0.36711001768278617
MergeBench/gemma-2-2b_math,multilingual,arc_ru,"acc_norm,none",0.3515825491873396
MergeBench/gemma-2-2b_math,multilingual,hellaswag_ru,"acc_norm,none",0.5360224331320104
MergeBench/gemma-2-2b_math,instruction_following,ifeval,"inst_level_loose_acc,none",0.2517985611510791
MergeBench/gemma-2-2b_math,coding,mbpp_plus,"pass_at_1,none",0.4365079365079365
MergeBench/gemma-2-2b_math,coding,humaneval_plus,"pass@1,create_test",0.15853658536585366
MergeBench/gemma-2-2b_math,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.36970169362877914
MergeBench/gemma-2-2b_math,safety,toxigen,"acc_norm,none",0.4319148936170213
MergeBench/gemma-2-2b_math,safety,winogender,"winogender_all:acc,none",0.5597222222222222
MergeBench/gemma-2-2b_coding,math,gsm8k_cot,"exact_match,flexible-extract",0.3191811978771797
MergeBench/gemma-2-2b_coding,multilingual,m_mmlu_fr,"acc,none",0.4420594301428462
MergeBench/gemma-2-2b_coding,multilingual,arc_fr,"acc_norm,none",0.4251497005988024
MergeBench/gemma-2-2b_coding,multilingual,hellaswag_fr,"acc_norm,none",0.6197258513600342
MergeBench/gemma-2-2b_coding,multilingual,m_mmlu_es,"acc,none",0.447727613619319
MergeBench/gemma-2-2b_coding,multilingual,arc_es,"acc_norm,none",0.4641025641025641
MergeBench/gemma-2-2b_coding,multilingual,hellaswag_es,"acc_norm,none",0.6306806059312994
MergeBench/gemma-2-2b_coding,multilingual,m_mmlu_de,"acc,none",0.4408658922914467
MergeBench/gemma-2-2b_coding,multilingual,arc_de,"acc_norm,none",0.40547476475620187
MergeBench/gemma-2-2b_coding,multilingual,hellaswag_de,"acc_norm,none",0.5592442356959864
MergeBench/gemma-2-2b_coding,multilingual,m_mmlu_ru,"acc,none",0.3977858076420389
MergeBench/gemma-2-2b_coding,multilingual,arc_ru,"acc_norm,none",0.3781009409751925
MergeBench/gemma-2-2b_coding,multilingual,hellaswag_ru,"acc_norm,none",0.5504745470232959
MergeBench/gemma-2-2b_coding,instruction_following,ifeval,"inst_level_loose_acc,none",0.25539568345323743
MergeBench/gemma-2-2b_coding,coding,mbpp_plus,"pass_at_1,none",0.5185185185185185
MergeBench/gemma-2-2b_coding,coding,humaneval_plus,"pass@1,create_test",0.3170731707317073
MergeBench/gemma-2-2b_coding,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.4269271436547877
MergeBench/gemma-2-2b_coding,safety,toxigen,"acc_norm,none",0.4308510638297872
MergeBench/gemma-2-2b_coding,safety,winogender,"winogender_all:acc,none",0.5694444444444444
MergeBench/gemma-2-2b_multilingual,math,gsm8k_cot,"exact_match,flexible-extract",0.23805913570887036
MergeBench/gemma-2-2b_multilingual,multilingual,m_mmlu_fr,"acc,none",0.41906653426017876
MergeBench/gemma-2-2b_multilingual,multilingual,arc_fr,"acc_norm,none",0.40461933276304535
MergeBench/gemma-2-2b_multilingual,multilingual,hellaswag_fr,"acc_norm,none",0.5952023988005997
MergeBench/gemma-2-2b_multilingual,multilingual,m_mmlu_es,"acc,none",0.4169041547922604
MergeBench/gemma-2-2b_multilingual,multilingual,arc_es,"acc_norm,none",0.4170940170940171
MergeBench/gemma-2-2b_multilingual,multilingual,hellaswag_es,"acc_norm,none",0.6122253040324301
MergeBench/gemma-2-2b_multilingual,multilingual,m_mmlu_de,"acc,none",0.4177854880072409
MergeBench/gemma-2-2b_multilingual,multilingual,arc_de,"acc_norm,none",0.39349871685201027
MergeBench/gemma-2-2b_multilingual,multilingual,hellaswag_de,"acc_norm,none",0.5475021349274125
MergeBench/gemma-2-2b_multilingual,multilingual,m_mmlu_ru,"acc,none",0.36772507111555314
MergeBench/gemma-2-2b_multilingual,multilingual,arc_ru,"acc_norm,none",0.3823781009409752
MergeBench/gemma-2-2b_multilingual,multilingual,hellaswag_ru,"acc_norm,none",0.5391501294219154
MergeBench/gemma-2-2b_multilingual,instruction_following,ifeval,"inst_level_loose_acc,none",0.1606714628297362
MergeBench/gemma-2-2b_multilingual,coding,mbpp_plus,"pass_at_1,none",0.37566137566137564
MergeBench/gemma-2-2b_multilingual,coding,humaneval_plus,"pass@1,create_test",0.18902439024390244
MergeBench/gemma-2-2b_multilingual,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.398198943413569
MergeBench/gemma-2-2b_multilingual,safety,toxigen,"acc_norm,none",0.44148936170212766
MergeBench/gemma-2-2b_multilingual,safety,winogender,"winogender_all:acc,none",0.5777777777777777
MergeBench/gemma-2-2b_safety,math,gsm8k_cot,"exact_match,flexible-extract",0.30477634571645185
MergeBench/gemma-2-2b_safety,multilingual,m_mmlu_fr,"acc,none",0.4001222213734627
MergeBench/gemma-2-2b_safety,multilingual,arc_fr,"acc_norm,none",0.40119760479041916
MergeBench/gemma-2-2b_safety,multilingual,hellaswag_fr,"acc_norm,none",0.6083743842364532
MergeBench/gemma-2-2b_safety,multilingual,m_mmlu_es,"acc,none",0.41097945102744865
MergeBench/gemma-2-2b_safety,multilingual,arc_es,"acc_norm,none",0.4094017094017094
MergeBench/gemma-2-2b_safety,multilingual,hellaswag_es,"acc_norm,none",0.6161723917217836
MergeBench/gemma-2-2b_safety,multilingual,m_mmlu_de,"acc,none",0.40782923517876
MergeBench/gemma-2-2b_safety,multilingual,arc_de,"acc_norm,none",0.3849443969204448
MergeBench/gemma-2-2b_safety,multilingual,hellaswag_de,"acc_norm,none",0.5454739538855679
MergeBench/gemma-2-2b_safety,multilingual,m_mmlu_ru,"acc,none",0.37095410163757975
MergeBench/gemma-2-2b_safety,multilingual,arc_ru,"acc_norm,none",0.3558597091531223
MergeBench/gemma-2-2b_safety,multilingual,hellaswag_ru,"acc_norm,none",0.5348360655737705
MergeBench/gemma-2-2b_safety,instruction_following,ifeval,"inst_level_loose_acc,none",0.14268585131894485
MergeBench/gemma-2-2b_safety,coding,mbpp_plus,"pass_at_1,none",0.47354497354497355
MergeBench/gemma-2-2b_safety,coding,humaneval_plus,"pass@1,create_test",0.17682926829268292
MergeBench/gemma-2-2b_safety,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.4560928301633455
MergeBench/gemma-2-2b_safety,safety,toxigen,"acc_norm,none",0.4319148936170213
MergeBench/gemma-2-2b_safety,safety,winogender,"winogender_all:acc,none",0.5722222222222222

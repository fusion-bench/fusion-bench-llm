model,task_category,task_name,metric_name,score
google/gemma-2-9b,math,gsm8k_cot,"exact_match,flexible-extract",0.7005307050796058
google/gemma-2-9b,multilingual,m_mmlu_fr,"acc,none",0.6130929646321901
google/gemma-2-9b,multilingual,arc_fr,"acc_norm,none",0.5851154833190761
google/gemma-2-9b,multilingual,hellaswag_fr,"acc_norm,none",0.7050760334118655
google/gemma-2-9b,multilingual,m_mmlu_es,"acc,none",0.6196190190490476
google/gemma-2-9b,multilingual,arc_es,"acc_norm,none",0.582905982905983
google/gemma-2-9b,multilingual,hellaswag_es,"acc_norm,none",0.7189033496906336
google/gemma-2-9b,multilingual,m_mmlu_de,"acc,none",0.6086136672197918
google/gemma-2-9b,multilingual,arc_de,"acc_norm,none",0.5431993156544055
google/gemma-2-9b,multilingual,hellaswag_de,"acc_norm,none",0.6658838599487618
google/gemma-2-9b,multilingual,m_mmlu_ru,"acc,none",0.5720765741523794
google/gemma-2-9b,multilingual,arc_ru,"acc_norm,none",0.5166809238665526
google/gemma-2-9b,multilingual,hellaswag_ru,"acc_norm,none",0.6416091458153581
google/gemma-2-9b,instruction_following,ifeval,"inst_level_loose_acc,none",0.2733812949640288
google/gemma-2-9b,coding,mbpp_plus,"pass_at_1,none",0.6587301587301587
google/gemma-2-9b,coding,humaneval_plus,"pass@1,create_test",0.34146341463414637
google/gemma-2-9b,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.4550525760129265
google/gemma-2-9b,safety,toxigen,"acc_norm,none",0.4308510638297872
google/gemma-2-9b,safety,winogender,"winogender_all:acc,none",0.625
MergeBench/gemma-2-9b_instruction,math,gsm8k_cot,"exact_match,flexible-extract",0.7498104624715694
MergeBench/gemma-2-9b_instruction,multilingual,m_mmlu_fr,"acc,none",0.6254678786952869
MergeBench/gemma-2-9b_instruction,multilingual,arc_fr,"acc_norm,none",0.572284003421728
MergeBench/gemma-2-9b_instruction,multilingual,hellaswag_fr,"acc_norm,none",0.7037909616620261
MergeBench/gemma-2-9b_instruction,multilingual,m_mmlu_es,"acc,none",0.6312434378281085
MergeBench/gemma-2-9b_instruction,multilingual,arc_es,"acc_norm,none",0.576068376068376
MergeBench/gemma-2-9b_instruction,multilingual,hellaswag_es,"acc_norm,none",0.7255173885214423
MergeBench/gemma-2-9b_instruction,multilingual,m_mmlu_de,"acc,none",0.6113290089002866
MergeBench/gemma-2-9b_instruction,multilingual,arc_de,"acc_norm,none",0.5226689478186484
MergeBench/gemma-2-9b_instruction,multilingual,hellaswag_de,"acc_norm,none",0.6664175918018788
MergeBench/gemma-2-9b_instruction,multilingual,m_mmlu_ru,"acc,none",0.5742292611670639
MergeBench/gemma-2-9b_instruction,multilingual,arc_ru,"acc_norm,none",0.5106928999144568
MergeBench/gemma-2-9b_instruction,multilingual,hellaswag_ru,"acc_norm,none",0.6473252804141502
MergeBench/gemma-2-9b_instruction,instruction_following,ifeval,"inst_level_loose_acc,none",0.7685851318944844
MergeBench/gemma-2-9b_instruction,coding,mbpp_plus,"pass_at_1,none",0.7063492063492064
MergeBench/gemma-2-9b_instruction,coding,humaneval_plus,"pass@1,create_test",0.4634146341463415
MergeBench/gemma-2-9b_instruction,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5752696878776983
MergeBench/gemma-2-9b_instruction,safety,toxigen,"acc_norm,none",0.4308510638297872
MergeBench/gemma-2-9b_instruction,safety,winogender,"winogender_all:acc,none",0.6111111111111112
MergeBench/gemma-2-9b_math,math,gsm8k_cot,"exact_match,flexible-extract",0.5633055344958302
MergeBench/gemma-2-9b_math,multilingual,m_mmlu_fr,"acc,none",0.412726300511802
MergeBench/gemma-2-9b_math,multilingual,arc_fr,"acc_norm,none",0.3857998289136014
MergeBench/gemma-2-9b_math,multilingual,hellaswag_fr,"acc_norm,none",0.6177982437352753
MergeBench/gemma-2-9b_math,multilingual,m_mmlu_es,"acc,none",0.4072296385180741
MergeBench/gemma-2-9b_math,multilingual,arc_es,"acc_norm,none",0.39914529914529917
MergeBench/gemma-2-9b_math,multilingual,hellaswag_es,"acc_norm,none",0.6233198207808833
MergeBench/gemma-2-9b_math,multilingual,m_mmlu_de,"acc,none",0.3844471262633881
MergeBench/gemma-2-9b_math,multilingual,arc_de,"acc_norm,none",0.34473909324208724
MergeBench/gemma-2-9b_math,multilingual,hellaswag_de,"acc_norm,none",0.5643680614859095
MergeBench/gemma-2-9b_math,multilingual,m_mmlu_ru,"acc,none",0.3465826093641885
MergeBench/gemma-2-9b_math,multilingual,arc_ru,"acc_norm,none",0.3438836612489307
MergeBench/gemma-2-9b_math,multilingual,hellaswag_ru,"acc_norm,none",0.517795513373598
MergeBench/gemma-2-9b_math,instruction_following,ifeval,"inst_level_loose_acc,none",0.21103117505995203
MergeBench/gemma-2-9b_math,coding,mbpp_plus,"pass_at_1,none",0.47354497354497355
MergeBench/gemma-2-9b_math,coding,humaneval_plus,"pass@1,create_test",0.024390243902439025
MergeBench/gemma-2-9b_math,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.4437575397890935
MergeBench/gemma-2-9b_math,safety,toxigen,"acc_norm,none",0.4308510638297872
MergeBench/gemma-2-9b_math,safety,winogender,"winogender_all:acc,none",0.6194444444444445
MergeBench/gemma-2-9b_coding,math,gsm8k_cot,"exact_match,flexible-extract",0.6072782410917361
MergeBench/gemma-2-9b_coding,multilingual,m_mmlu_fr,"acc,none",0.5644335803223589
MergeBench/gemma-2-9b_coding,multilingual,arc_fr,"acc_norm,none",0.47476475620188197
MergeBench/gemma-2-9b_coding,multilingual,hellaswag_fr,"acc_norm,none",0.7003641036624545
MergeBench/gemma-2-9b_coding,multilingual,m_mmlu_es,"acc,none",0.5663716814159292
MergeBench/gemma-2-9b_coding,multilingual,arc_es,"acc_norm,none",0.48717948717948717
MergeBench/gemma-2-9b_coding,multilingual,hellaswag_es,"acc_norm,none",0.718049925325368
MergeBench/gemma-2-9b_coding,multilingual,m_mmlu_de,"acc,none",0.5508372303514859
MergeBench/gemma-2-9b_coding,multilingual,arc_de,"acc_norm,none",0.4568006843455945
MergeBench/gemma-2-9b_coding,multilingual,hellaswag_de,"acc_norm,none",0.6720751494449189
MergeBench/gemma-2-9b_coding,multilingual,m_mmlu_ru,"acc,none",0.5246405781502268
MergeBench/gemma-2-9b_coding,multilingual,arc_ru,"acc_norm,none",0.43627031650983744
MergeBench/gemma-2-9b_coding,multilingual,hellaswag_ru,"acc_norm,none",0.6423641069887834
MergeBench/gemma-2-9b_coding,instruction_following,ifeval,"inst_level_loose_acc,none",0.38968824940047964
MergeBench/gemma-2-9b_coding,coding,mbpp_plus,"pass_at_1,none",0.6587301587301587
MergeBench/gemma-2-9b_coding,coding,humaneval_plus,"pass@1,create_test",0.5914634146341463
MergeBench/gemma-2-9b_coding,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5419506998265322
MergeBench/gemma-2-9b_coding,safety,toxigen,"acc_norm,none",0.4297872340425532
MergeBench/gemma-2-9b_coding,safety,winogender,"winogender_all:acc,none",0.6444444444444445
MergeBench/gemma-2-9b_multilingual,math,gsm8k_cot,"exact_match,flexible-extract",0.7119029567854435
MergeBench/gemma-2-9b_multilingual,multilingual,m_mmlu_fr,"acc,none",0.6213429073409212
MergeBench/gemma-2-9b_multilingual,multilingual,arc_fr,"acc_norm,none",0.5936698032506416
MergeBench/gemma-2-9b_multilingual,multilingual,hellaswag_fr,"acc_norm,none",0.7305632897836796
MergeBench/gemma-2-9b_multilingual,multilingual,m_mmlu_es,"acc,none",0.6146692665366732
MergeBench/gemma-2-9b_multilingual,multilingual,arc_es,"acc_norm,none",0.6111111111111112
MergeBench/gemma-2-9b_multilingual,multilingual,hellaswag_es,"acc_norm,none",0.7453595050138682
MergeBench/gemma-2-9b_multilingual,multilingual,m_mmlu_de,"acc,none",0.6101221903756223
MergeBench/gemma-2-9b_multilingual,multilingual,arc_de,"acc_norm,none",0.5508982035928144
MergeBench/gemma-2-9b_multilingual,multilingual,hellaswag_de,"acc_norm,none",0.696199829205807
MergeBench/gemma-2-9b_multilingual,multilingual,m_mmlu_ru,"acc,none",0.5650803413546552
MergeBench/gemma-2-9b_multilingual,multilingual,arc_ru,"acc_norm,none",0.5431993156544055
MergeBench/gemma-2-9b_multilingual,multilingual,hellaswag_ru,"acc_norm,none",0.6663071613459879
MergeBench/gemma-2-9b_multilingual,instruction_following,ifeval,"inst_level_loose_acc,none",0.20503597122302158
MergeBench/gemma-2-9b_multilingual,coding,mbpp_plus,"pass_at_1,none",0.6851851851851852
MergeBench/gemma-2-9b_multilingual,coding,humaneval_plus,"pass@1,create_test",0.3231707317073171
MergeBench/gemma-2-9b_multilingual,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.4990723869570329
MergeBench/gemma-2-9b_multilingual,safety,toxigen,"acc_norm,none",0.4308510638297872
MergeBench/gemma-2-9b_multilingual,safety,winogender,"winogender_all:acc,none",0.6527777777777778
MergeBench/gemma-2-9b_safety,math,gsm8k_cot,"exact_match,flexible-extract",0.604245640636846
MergeBench/gemma-2-9b_safety,multilingual,m_mmlu_fr,"acc,none",0.41952486441066383
MergeBench/gemma-2-9b_safety,multilingual,arc_fr,"acc_norm,none",0.42001710863986313
MergeBench/gemma-2-9b_safety,multilingual,hellaswag_fr,"acc_norm,none",0.6479974298565003
MergeBench/gemma-2-9b_safety,multilingual,m_mmlu_es,"acc,none",0.44015299235038247
MergeBench/gemma-2-9b_safety,multilingual,arc_es,"acc_norm,none",0.4358974358974359
MergeBench/gemma-2-9b_safety,multilingual,hellaswag_es,"acc_norm,none",0.6637508000853425
MergeBench/gemma-2-9b_safety,multilingual,m_mmlu_de,"acc,none",0.3948559360386182
MergeBench/gemma-2-9b_safety,multilingual,arc_de,"acc_norm,none",0.40975192472198463
MergeBench/gemma-2-9b_safety,multilingual,hellaswag_de,"acc_norm,none",0.6024765157984628
MergeBench/gemma-2-9b_safety,multilingual,m_mmlu_ru,"acc,none",0.4039363419697086
MergeBench/gemma-2-9b_safety,multilingual,arc_ru,"acc_norm,none",0.41659538066723695
MergeBench/gemma-2-9b_safety,multilingual,hellaswag_ru,"acc_norm,none",0.5907031924072477
MergeBench/gemma-2-9b_safety,instruction_following,ifeval,"inst_level_loose_acc,none",0.22062350119904076
MergeBench/gemma-2-9b_safety,coding,mbpp_plus,"pass_at_1,none",0.6031746031746031
MergeBench/gemma-2-9b_safety,coding,humaneval_plus,"pass@1,create_test",0.3353658536585366
MergeBench/gemma-2-9b_safety,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5140244829478448
MergeBench/gemma-2-9b_safety,safety,toxigen,"acc_norm,none",0.4319148936170213
MergeBench/gemma-2-9b_safety,safety,winogender,"winogender_all:acc,none",0.6388888888888888

model,task_category,task_name,metric_name,score
meta-llama/Llama-3.1-8B,math,gsm8k_cot,"exact_match,flexible-extract",0.5701288855193328
meta-llama/Llama-3.1-8B,multilingual,m_mmlu_fr,"acc,none",0.5394545871209228
meta-llama/Llama-3.1-8B,multilingual,arc_fr,"acc_norm,none",0.46877673224978617
meta-llama/Llama-3.1-8B,multilingual,hellaswag_fr,"acc_norm,none",0.6523880916684515
meta-llama/Llama-3.1-8B,multilingual,m_mmlu_es,"acc,none",0.556697165141743
meta-llama/Llama-3.1-8B,multilingual,arc_es,"acc_norm,none",0.47094017094017093
meta-llama/Llama-3.1-8B,multilingual,hellaswag_es,"acc_norm,none",0.6782590142948581
meta-llama/Llama-3.1-8B,multilingual,m_mmlu_de,"acc,none",0.5260974505958667
meta-llama/Llama-3.1-8B,multilingual,arc_de,"acc_norm,none",0.437125748502994
meta-llama/Llama-3.1-8B,multilingual,hellaswag_de,"acc_norm,none",0.607600341588386
meta-llama/Llama-3.1-8B,multilingual,m_mmlu_ru,"acc,none",0.5098024140847236
meta-llama/Llama-3.1-8B,multilingual,arc_ru,"acc_norm,none",0.42172797262617623
meta-llama/Llama-3.1-8B,multilingual,hellaswag_ru,"acc_norm,none",0.584879206212252
meta-llama/Llama-3.1-8B,instruction_following,ifeval,"inst_level_loose_acc,none",0.18225419664268586
meta-llama/Llama-3.1-8B,coding,mbpp_plus,"pass_at_1,none",0.6296296296296297
meta-llama/Llama-3.1-8B,coding,humaneval_plus,"pass@1,create_test",0.2865853658536585
meta-llama/Llama-3.1-8B,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.45129164824231466
meta-llama/Llama-3.1-8B,safety,toxigen,"acc_norm,none",0.4319148936170213
meta-llama/Llama-3.1-8B,safety,winogender,"winogender_all:acc,none",0.6069444444444444
meta-llama/Llama-3.1-8B-Instruct,math,gsm8k_cot,"exact_match,flexible-extract",0.7952994692949203
meta-llama/Llama-3.1-8B-Instruct,multilingual,m_mmlu_fr,"acc,none",0.48995493086853564
meta-llama/Llama-3.1-8B-Instruct,multilingual,arc_fr,"acc_norm,none",0.4439692044482464
meta-llama/Llama-3.1-8B-Instruct,multilingual,hellaswag_fr,"acc_norm,none",0.603448275862069
meta-llama/Llama-3.1-8B-Instruct,multilingual,m_mmlu_es,"acc,none",0.5497975101244937
meta-llama/Llama-3.1-8B-Instruct,multilingual,arc_es,"acc_norm,none",0.4478632478632479
meta-llama/Llama-3.1-8B-Instruct,multilingual,hellaswag_es,"acc_norm,none",0.6104117772562406
meta-llama/Llama-3.1-8B-Instruct,multilingual,m_mmlu_de,"acc,none",0.489892894855936
meta-llama/Llama-3.1-8B-Instruct,multilingual,arc_de,"acc_norm,none",0.43028229255774164
meta-llama/Llama-3.1-8B-Instruct,multilingual,hellaswag_de,"acc_norm,none",0.5778181041844578
meta-llama/Llama-3.1-8B-Instruct,multilingual,m_mmlu_ru,"acc,none",0.4073960175290228
meta-llama/Llama-3.1-8B-Instruct,multilingual,arc_ru,"acc_norm,none",0.39692044482463645
meta-llama/Llama-3.1-8B-Instruct,multilingual,hellaswag_ru,"acc_norm,none",0.5545729076790337
meta-llama/Llama-3.1-8B-Instruct,instruction_following,ifeval,"inst_level_loose_acc,none",0.854916067146283
meta-llama/Llama-3.1-8B-Instruct,coding,mbpp_plus,"pass_at_1,none",0.0
meta-llama/Llama-3.1-8B-Instruct,coding,humaneval_plus,"pass@1,create_test",0.006097560975609756
meta-llama/Llama-3.1-8B-Instruct,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5511422642920761
meta-llama/Llama-3.1-8B-Instruct,safety,toxigen,"acc_norm,none",0.8425531914893617
meta-llama/Llama-3.1-8B-Instruct,safety,winogender,"winogender_all:acc,none",0.6944444444444444
mlfoundations-dev/seed_math_allenai_math,math,gsm8k_cot,"exact_match,flexible-extract",0.6770280515542078
mlfoundations-dev/seed_math_allenai_math,multilingual,m_mmlu_fr,"acc,none",0.5395309754793369
mlfoundations-dev/seed_math_allenai_math,multilingual,arc_fr,"acc_norm,none",0.44568006843455943
mlfoundations-dev/seed_math_allenai_math,multilingual,hellaswag_fr,"acc_norm,none",0.6595630756050546
mlfoundations-dev/seed_math_allenai_math,multilingual,m_mmlu_es,"acc,none",0.5520473976301185
mlfoundations-dev/seed_math_allenai_math,multilingual,arc_es,"acc_norm,none",0.44188034188034186
mlfoundations-dev/seed_math_allenai_math,multilingual,hellaswag_es,"acc_norm,none",0.6858331555365905
mlfoundations-dev/seed_math_allenai_math,multilingual,m_mmlu_de,"acc,none",0.5246643535978277
mlfoundations-dev/seed_math_allenai_math,multilingual,arc_de,"acc_norm,none",0.43028229255774164
mlfoundations-dev/seed_math_allenai_math,multilingual,hellaswag_de,"acc_norm,none",0.6184884713919727
mlfoundations-dev/seed_math_allenai_math,multilingual,m_mmlu_ru,"acc,none",0.5104943491965864
mlfoundations-dev/seed_math_allenai_math,multilingual,arc_ru,"acc_norm,none",0.41402908468776733
mlfoundations-dev/seed_math_allenai_math,multilingual,hellaswag_ru,"acc_norm,none",0.5939387402933564
mlfoundations-dev/seed_math_allenai_math,instruction_following,ifeval,"inst_level_loose_acc,none",0.3009592326139089
mlfoundations-dev/seed_math_allenai_math,coding,mbpp_plus,"pass_at_1,none",0.6587301587301587
mlfoundations-dev/seed_math_allenai_math,coding,humaneval_plus,"pass@1,create_test",0.40853658536585363
mlfoundations-dev/seed_math_allenai_math,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5467308774109544
mlfoundations-dev/seed_math_allenai_math,safety,toxigen,"acc_norm,none",0.4319148936170213
mlfoundations-dev/seed_math_allenai_math,safety,winogender,"winogender_all:acc,none",0.625
MergeBench/Llama-3.1-8B_coding,math,gsm8k_cot,"exact_match,flexible-extract",0.5193328278999242
MergeBench/Llama-3.1-8B_coding,multilingual,m_mmlu_fr,"acc,none",0.5105797876403636
MergeBench/Llama-3.1-8B_coding,multilingual,arc_fr,"acc_norm,none",0.4593669803250642
MergeBench/Llama-3.1-8B_coding,multilingual,hellaswag_fr,"acc_norm,none",0.6647033626044121
MergeBench/Llama-3.1-8B_coding,multilingual,m_mmlu_es,"acc,none",0.5183740812959352
MergeBench/Llama-3.1-8B_coding,multilingual,arc_es,"acc_norm,none",0.4478632478632479
MergeBench/Llama-3.1-8B_coding,multilingual,hellaswag_es,"acc_norm,none",0.6886067847237038
MergeBench/Llama-3.1-8B_coding,multilingual,m_mmlu_de,"acc,none",0.49653039674159
MergeBench/Llama-3.1-8B_coding,multilingual,arc_de,"acc_norm,none",0.4234388366124893
MergeBench/Llama-3.1-8B_coding,multilingual,hellaswag_de,"acc_norm,none",0.6247865072587532
MergeBench/Llama-3.1-8B_coding,multilingual,m_mmlu_ru,"acc,none",0.4708234027831168
MergeBench/Llama-3.1-8B_coding,multilingual,arc_ru,"acc_norm,none",0.4251497005988024
MergeBench/Llama-3.1-8B_coding,multilingual,hellaswag_ru,"acc_norm,none",0.6027825711820535
MergeBench/Llama-3.1-8B_coding,instruction_following,ifeval,"inst_level_loose_acc,none",0.34652278177458035
MergeBench/Llama-3.1-8B_coding,coding,mbpp_plus,"pass_at_1,none",0.6428571428571429
MergeBench/Llama-3.1-8B_coding,coding,humaneval_plus,"pass@1,create_test",0.5304878048780488
MergeBench/Llama-3.1-8B_coding,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.48669500557621853
MergeBench/Llama-3.1-8B_coding,safety,toxigen,"acc_norm,none",0.4319148936170213
MergeBench/Llama-3.1-8B_coding,safety,winogender,"winogender_all:acc,none",0.6333333333333333
MergeBench/Llama-3.1-8B_multilingual,math,gsm8k_cot,"exact_match,flexible-extract",0.5519332827899924
MergeBench/Llama-3.1-8B_multilingual,multilingual,m_mmlu_fr,"acc,none",0.5495378504315942
MergeBench/Llama-3.1-8B_multilingual,multilingual,arc_fr,"acc_norm,none",0.49016253207869975
MergeBench/Llama-3.1-8B_multilingual,multilingual,hellaswag_fr,"acc_norm,none",0.6774469907903191
MergeBench/Llama-3.1-8B_multilingual,multilingual,m_mmlu_es,"acc,none",0.555797210139493
MergeBench/Llama-3.1-8B_multilingual,multilingual,arc_es,"acc_norm,none",0.4931623931623932
MergeBench/Llama-3.1-8B_multilingual,multilingual,hellaswag_es,"acc_norm,none",0.6983144868786004
MergeBench/Llama-3.1-8B_multilingual,multilingual,m_mmlu_de,"acc,none",0.5391461758938
MergeBench/Llama-3.1-8B_multilingual,multilingual,arc_de,"acc_norm,none",0.4619332763045338
MergeBench/Llama-3.1-8B_multilingual,multilingual,hellaswag_de,"acc_norm,none",0.6330059777967549
MergeBench/Llama-3.1-8B_multilingual,multilingual,m_mmlu_ru,"acc,none",0.5164911201660645
MergeBench/Llama-3.1-8B_multilingual,multilingual,arc_ru,"acc_norm,none",0.4533789563729683
MergeBench/Llama-3.1-8B_multilingual,multilingual,hellaswag_ru,"acc_norm,none",0.604076790336497
MergeBench/Llama-3.1-8B_multilingual,instruction_following,ifeval,"inst_level_loose_acc,none",0.02877697841726619
MergeBench/Llama-3.1-8B_multilingual,coding,mbpp_plus,"pass_at_1,none",0.6481481481481481
MergeBench/Llama-3.1-8B_multilingual,coding,humaneval_plus,"pass@1,create_test",0.3231707317073171
MergeBench/Llama-3.1-8B_multilingual,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5130748916396661
MergeBench/Llama-3.1-8B_multilingual,safety,toxigen,"acc_norm,none",0.4297872340425532
MergeBench/Llama-3.1-8B_multilingual,safety,winogender,"winogender_all:acc,none",0.6180555555555556

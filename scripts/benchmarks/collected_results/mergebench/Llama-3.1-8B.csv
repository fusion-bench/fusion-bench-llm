model,task_category,task_name,metric_name,score
meta-llama/Llama-3.1-8B,math,gsm8k_cot,"exact_match,flexible-extract",0.5701288855193328
meta-llama/Llama-3.1-8B,multilingual,m_mmlu_fr,"acc,none",0.5394545871209228
meta-llama/Llama-3.1-8B,multilingual,arc_fr,"acc_norm,none",0.46877673224978617
meta-llama/Llama-3.1-8B,multilingual,hellaswag_fr,"acc_norm,none",0.6523880916684515
meta-llama/Llama-3.1-8B,multilingual,m_mmlu_es,"acc,none",0.556697165141743
meta-llama/Llama-3.1-8B,multilingual,arc_es,"acc_norm,none",0.47094017094017093
meta-llama/Llama-3.1-8B,multilingual,hellaswag_es,"acc_norm,none",0.6782590142948581
meta-llama/Llama-3.1-8B,multilingual,m_mmlu_de,"acc,none",0.5260974505958667
meta-llama/Llama-3.1-8B,multilingual,arc_de,"acc_norm,none",0.437125748502994
meta-llama/Llama-3.1-8B,multilingual,hellaswag_de,"acc_norm,none",0.607600341588386
meta-llama/Llama-3.1-8B,multilingual,m_mmlu_ru,"acc,none",0.5098024140847236
meta-llama/Llama-3.1-8B,multilingual,arc_ru,"acc_norm,none",0.42172797262617623
meta-llama/Llama-3.1-8B,multilingual,hellaswag_ru,"acc_norm,none",0.584879206212252
meta-llama/Llama-3.1-8B,instruction_following,ifeval,"inst_level_loose_acc,none",0.18225419664268586
meta-llama/Llama-3.1-8B,coding,mbpp_plus,"pass_at_1,none",0.6296296296296297
meta-llama/Llama-3.1-8B,coding,humaneval_plus,"pass@1,create_test",0.2865853658536585
meta-llama/Llama-3.1-8B,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.45129164824231466
meta-llama/Llama-3.1-8B,safety,toxigen,"acc_norm,none",0.4319148936170213
meta-llama/Llama-3.1-8B,safety,winogender,"winogender_all:acc,none",0.6069444444444444
MergeBench/Llama-3.1-8B_instruction,math,gsm8k_cot,"exact_match,flexible-extract",0.6004548900682335
MergeBench/Llama-3.1-8B_instruction,multilingual,m_mmlu_fr,"acc,none",0.5024062332900466
MergeBench/Llama-3.1-8B_instruction,multilingual,arc_fr,"acc_norm,none",0.4644995722840034
MergeBench/Llama-3.1-8B_instruction,multilingual,hellaswag_fr,"acc_norm,none",0.6638466481045192
MergeBench/Llama-3.1-8B_instruction,multilingual,m_mmlu_es,"acc,none",0.5200989950502475
MergeBench/Llama-3.1-8B_instruction,multilingual,arc_es,"acc_norm,none",0.4598290598290598
MergeBench/Llama-3.1-8B_instruction,multilingual,hellaswag_es,"acc_norm,none",0.6888201408150203
MergeBench/Llama-3.1-8B_instruction,multilingual,m_mmlu_de,"acc,none",0.4904963041182682
MergeBench/Llama-3.1-8B_instruction,multilingual,arc_de,"acc_norm,none",0.43969204448246363
MergeBench/Llama-3.1-8B_instruction,multilingual,hellaswag_de,"acc_norm,none",0.6144321093082835
MergeBench/Llama-3.1-8B_instruction,multilingual,m_mmlu_ru,"acc,none",0.48781425386330435
MergeBench/Llama-3.1-8B_instruction,multilingual,arc_ru,"acc_norm,none",0.4568006843455945
MergeBench/Llama-3.1-8B_instruction,multilingual,hellaswag_ru,"acc_norm,none",0.60148835202761
MergeBench/Llama-3.1-8B_instruction,instruction_following,ifeval,"inst_level_loose_acc,none",0.47601918465227816
MergeBench/Llama-3.1-8B_instruction,coding,mbpp_plus,"pass_at_1,none",0.6402116402116402
MergeBench/Llama-3.1-8B_instruction,coding,humaneval_plus,"pass@1,create_test",0.34146341463414637
MergeBench/Llama-3.1-8B_instruction,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5979682351952592
MergeBench/Llama-3.1-8B_instruction,safety,toxigen,"acc_norm,none",0.4319148936170213
MergeBench/Llama-3.1-8B_instruction,safety,winogender,"winogender_all:acc,none",0.6180555555555556
MergeBench/Llama-3.1-8B_math,math,gsm8k_cot,"exact_match,flexible-extract",0.3995451099317665
MergeBench/Llama-3.1-8B_math,multilingual,m_mmlu_fr,"acc,none",0.46161484989687573
MergeBench/Llama-3.1-8B_math,multilingual,arc_fr,"acc_norm,none",0.39264328485885375
MergeBench/Llama-3.1-8B_math,multilingual,hellaswag_fr,"acc_norm,none",0.6421075176697365
MergeBench/Llama-3.1-8B_math,multilingual,m_mmlu_es,"acc,none",0.47472626368681564
MergeBench/Llama-3.1-8B_math,multilingual,arc_es,"acc_norm,none",0.382051282051282
MergeBench/Llama-3.1-8B_math,multilingual,hellaswag_es,"acc_norm,none",0.6532963516108385
MergeBench/Llama-3.1-8B_math,multilingual,m_mmlu_de,"acc,none",0.4479559511238497
MergeBench/Llama-3.1-8B_math,multilingual,arc_de,"acc_norm,none",0.3550042771599658
MergeBench/Llama-3.1-8B_math,multilingual,hellaswag_de,"acc_norm,none",0.5894534585824082
MergeBench/Llama-3.1-8B_math,multilingual,m_mmlu_ru,"acc,none",0.42215729991543016
MergeBench/Llama-3.1-8B_math,multilingual,arc_ru,"acc_norm,none",0.358426005132592
MergeBench/Llama-3.1-8B_math,multilingual,hellaswag_ru,"acc_norm,none",0.568809318377912
MergeBench/Llama-3.1-8B_math,instruction_following,ifeval,"inst_level_loose_acc,none",0.2637889688249401
MergeBench/Llama-3.1-8B_math,coding,mbpp_plus,"pass_at_1,none",0.5317460317460317
MergeBench/Llama-3.1-8B_math,coding,humaneval_plus,"pass@1,create_test",0.18902439024390244
MergeBench/Llama-3.1-8B_math,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.443554853409112
MergeBench/Llama-3.1-8B_math,safety,toxigen,"acc_norm,none",0.4319148936170213
MergeBench/Llama-3.1-8B_math,safety,winogender,"winogender_all:acc,none",0.6430555555555556
MergeBench/Llama-3.1-8B_coding,math,gsm8k_cot,"exact_match,flexible-extract",0.5193328278999242
MergeBench/Llama-3.1-8B_coding,multilingual,m_mmlu_fr,"acc,none",0.5105797876403636
MergeBench/Llama-3.1-8B_coding,multilingual,arc_fr,"acc_norm,none",0.4593669803250642
MergeBench/Llama-3.1-8B_coding,multilingual,hellaswag_fr,"acc_norm,none",0.6647033626044121
MergeBench/Llama-3.1-8B_coding,multilingual,m_mmlu_es,"acc,none",0.5183740812959352
MergeBench/Llama-3.1-8B_coding,multilingual,arc_es,"acc_norm,none",0.4478632478632479
MergeBench/Llama-3.1-8B_coding,multilingual,hellaswag_es,"acc_norm,none",0.6886067847237038
MergeBench/Llama-3.1-8B_coding,multilingual,m_mmlu_de,"acc,none",0.49653039674159
MergeBench/Llama-3.1-8B_coding,multilingual,arc_de,"acc_norm,none",0.4234388366124893
MergeBench/Llama-3.1-8B_coding,multilingual,hellaswag_de,"acc_norm,none",0.6247865072587532
MergeBench/Llama-3.1-8B_coding,multilingual,m_mmlu_ru,"acc,none",0.4708234027831168
MergeBench/Llama-3.1-8B_coding,multilingual,arc_ru,"acc_norm,none",0.4251497005988024
MergeBench/Llama-3.1-8B_coding,multilingual,hellaswag_ru,"acc_norm,none",0.6027825711820535
MergeBench/Llama-3.1-8B_coding,instruction_following,ifeval,"inst_level_loose_acc,none",0.34652278177458035
MergeBench/Llama-3.1-8B_coding,coding,mbpp_plus,"pass_at_1,none",0.6428571428571429
MergeBench/Llama-3.1-8B_coding,coding,humaneval_plus,"pass@1,create_test",0.5304878048780488
MergeBench/Llama-3.1-8B_coding,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.48669500557621853
MergeBench/Llama-3.1-8B_coding,safety,toxigen,"acc_norm,none",0.4319148936170213
MergeBench/Llama-3.1-8B_coding,safety,winogender,"winogender_all:acc,none",0.6333333333333333
MergeBench/Llama-3.1-8B_multilingual,math,gsm8k_cot,"exact_match,flexible-extract",0.5519332827899924
MergeBench/Llama-3.1-8B_multilingual,multilingual,m_mmlu_fr,"acc,none",0.5495378504315942
MergeBench/Llama-3.1-8B_multilingual,multilingual,arc_fr,"acc_norm,none",0.49016253207869975
MergeBench/Llama-3.1-8B_multilingual,multilingual,hellaswag_fr,"acc_norm,none",0.6774469907903191
MergeBench/Llama-3.1-8B_multilingual,multilingual,m_mmlu_es,"acc,none",0.555797210139493
MergeBench/Llama-3.1-8B_multilingual,multilingual,arc_es,"acc_norm,none",0.4931623931623932
MergeBench/Llama-3.1-8B_multilingual,multilingual,hellaswag_es,"acc_norm,none",0.6983144868786004
MergeBench/Llama-3.1-8B_multilingual,multilingual,m_mmlu_de,"acc,none",0.5391461758938
MergeBench/Llama-3.1-8B_multilingual,multilingual,arc_de,"acc_norm,none",0.4619332763045338
MergeBench/Llama-3.1-8B_multilingual,multilingual,hellaswag_de,"acc_norm,none",0.6330059777967549
MergeBench/Llama-3.1-8B_multilingual,multilingual,m_mmlu_ru,"acc,none",0.5164911201660645
MergeBench/Llama-3.1-8B_multilingual,multilingual,arc_ru,"acc_norm,none",0.4533789563729683
MergeBench/Llama-3.1-8B_multilingual,multilingual,hellaswag_ru,"acc_norm,none",0.604076790336497
MergeBench/Llama-3.1-8B_multilingual,instruction_following,ifeval,"inst_level_loose_acc,none",0.02877697841726619
MergeBench/Llama-3.1-8B_multilingual,coding,mbpp_plus,"pass_at_1,none",0.6481481481481481
MergeBench/Llama-3.1-8B_multilingual,coding,humaneval_plus,"pass@1,create_test",0.3231707317073171
MergeBench/Llama-3.1-8B_multilingual,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5130748916396661
MergeBench/Llama-3.1-8B_multilingual,safety,toxigen,"acc_norm,none",0.4297872340425532
MergeBench/Llama-3.1-8B_multilingual,safety,winogender,"winogender_all:acc,none",0.6180555555555556
MergeBench/Llama-3.1-8B_safety,math,gsm8k_cot,"exact_match,flexible-extract",0.5246398786959818
MergeBench/Llama-3.1-8B_safety,multilingual,m_mmlu_fr,"acc,none",0.43900389580627913
MergeBench/Llama-3.1-8B_safety,multilingual,arc_fr,"acc_norm,none",0.40975192472198463
MergeBench/Llama-3.1-8B_safety,multilingual,hellaswag_fr,"acc_norm,none",0.6240094238594989
MergeBench/Llama-3.1-8B_safety,multilingual,m_mmlu_es,"acc,none",0.44840257987100646
MergeBench/Llama-3.1-8B_safety,multilingual,arc_es,"acc_norm,none",0.41367521367521365
MergeBench/Llama-3.1-8B_safety,multilingual,hellaswag_es,"acc_norm,none",0.6395348837209303
MergeBench/Llama-3.1-8B_safety,multilingual,m_mmlu_de,"acc,none",0.42510182531301854
MergeBench/Llama-3.1-8B_safety,multilingual,arc_de,"acc_norm,none",0.3960650128314799
MergeBench/Llama-3.1-8B_safety,multilingual,hellaswag_de,"acc_norm,none",0.5794192997438087
MergeBench/Llama-3.1-8B_safety,multilingual,m_mmlu_ru,"acc,none",0.4193126777888829
MergeBench/Llama-3.1-8B_safety,multilingual,arc_ru,"acc_norm,none",0.39520958083832336
MergeBench/Llama-3.1-8B_safety,multilingual,hellaswag_ru,"acc_norm,none",0.5652502157031924
MergeBench/Llama-3.1-8B_safety,instruction_following,ifeval,"inst_level_loose_acc,none",0.046762589928057555
MergeBench/Llama-3.1-8B_safety,coding,mbpp_plus,"pass_at_1,none",0.6164021164021164
MergeBench/Llama-3.1-8B_safety,coding,humaneval_plus,"pass@1,create_test",0.3231707317073171
MergeBench/Llama-3.1-8B_safety,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5511666906423927
MergeBench/Llama-3.1-8B_safety,safety,toxigen,"acc_norm,none",0.4319148936170213
MergeBench/Llama-3.1-8B_safety,safety,winogender,"winogender_all:acc,none",0.6333333333333333

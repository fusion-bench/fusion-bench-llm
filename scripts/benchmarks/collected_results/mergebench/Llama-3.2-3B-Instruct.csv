model,task_category,task_name,metric_name,score
meta-llama/Llama-3.2-3B-Instruct,math,gsm8k_cot,"exact_match,flexible-extract",0.690674753601213
meta-llama/Llama-3.2-3B-Instruct,multilingual,m_mmlu_fr,"acc,none",0.41761515545030936
meta-llama/Llama-3.2-3B-Instruct,multilingual,arc_fr,"acc_norm,none",0.3729683490162532
meta-llama/Llama-3.2-3B-Instruct,multilingual,hellaswag_fr,"acc_norm,none",0.526343970871707
meta-llama/Llama-3.2-3B-Instruct,multilingual,m_mmlu_es,"acc,none",0.3574321283935803
meta-llama/Llama-3.2-3B-Instruct,multilingual,arc_es,"acc_norm,none",0.37094017094017095
meta-llama/Llama-3.2-3B-Instruct,multilingual,hellaswag_es,"acc_norm,none",0.5369106037977385
meta-llama/Llama-3.2-3B-Instruct,multilingual,m_mmlu_de,"acc,none",0.3143007995172726
meta-llama/Llama-3.2-3B-Instruct,multilingual,arc_de,"acc_norm,none",0.34901625320787
meta-llama/Llama-3.2-3B-Instruct,multilingual,hellaswag_de,"acc_norm,none",0.484201537147737
meta-llama/Llama-3.2-3B-Instruct,multilingual,m_mmlu_ru,"acc,none",0.4063965557007765
meta-llama/Llama-3.2-3B-Instruct,multilingual,arc_ru,"acc_norm,none",0.33704020530367834
meta-llama/Llama-3.2-3B-Instruct,multilingual,hellaswag_ru,"acc_norm,none",0.46537963761863677
meta-llama/Llama-3.2-3B-Instruct,instruction_following,ifeval,"inst_level_loose_acc,none",0.8249400479616307
meta-llama/Llama-3.2-3B-Instruct,coding,mbpp_plus,"pass_at_1,none",0.0
meta-llama/Llama-3.2-3B-Instruct,coding,humaneval_plus,"pass@1,create_test",0.03048780487804878
meta-llama/Llama-3.2-3B-Instruct,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5060229719446135
meta-llama/Llama-3.2-3B-Instruct,safety,toxigen,"acc_norm,none",0.8351063829787234
meta-llama/Llama-3.2-3B-Instruct,safety,winogender,"winogender_all:acc,none",0.6041666666666666
MergeBench/Llama-3.2-3B-Instruct_instruction,math,gsm8k_cot,"exact_match,flexible-extract",0.6717210007581501
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,m_mmlu_fr,"acc,none",0.4327400504163166
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,arc_fr,"acc_norm,none",0.3686911890504705
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,hellaswag_fr,"acc_norm,none",0.5189548083101306
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,m_mmlu_es,"acc,none",0.41697915104244787
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,arc_es,"acc_norm,none",0.3658119658119658
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,hellaswag_es,"acc_norm,none",0.5406443353957755
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,m_mmlu_de,"acc,none",0.3269723940262483
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,arc_de,"acc_norm,none",0.35072711719418304
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,hellaswag_de,"acc_norm,none",0.4917805294619983
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,m_mmlu_ru,"acc,none",0.395325593910971
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,arc_ru,"acc_norm,none",0.32763045337895635
MergeBench/Llama-3.2-3B-Instruct_instruction,multilingual,hellaswag_ru,"acc_norm,none",0.464732528041415
MergeBench/Llama-3.2-3B-Instruct_instruction,instruction_following,ifeval,"inst_level_loose_acc,none",0.8201438848920863
MergeBench/Llama-3.2-3B-Instruct_instruction,coding,mbpp_plus,"pass_at_1,none",0.08465608465608465
MergeBench/Llama-3.2-3B-Instruct_instruction,coding,humaneval_plus,"pass@1,create_test",0.0
MergeBench/Llama-3.2-3B-Instruct_instruction,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.49591035472022016
MergeBench/Llama-3.2-3B-Instruct_instruction,safety,toxigen,"acc_norm,none",0.8180851063829787
MergeBench/Llama-3.2-3B-Instruct_instruction,safety,winogender,"winogender_all:acc,none",0.5875
MergeBench/Llama-3.2-3B-Instruct_math,math,gsm8k_cot,"exact_match,flexible-extract",0.6611068991660348
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,m_mmlu_fr,"acc,none",0.39966389122297763
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,arc_fr,"acc_norm,none",0.3609923011120616
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,hellaswag_fr,"acc_norm,none",0.537159991432855
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,m_mmlu_es,"acc,none",0.40850457477126145
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,arc_es,"acc_norm,none",0.36324786324786323
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,hellaswag_es,"acc_norm,none",0.552912310646469
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,m_mmlu_de,"acc,none",0.3641574898174687
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,arc_de,"acc_norm,none",0.33361847733105215
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,hellaswag_de,"acc_norm,none",0.49402220324508966
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,m_mmlu_ru,"acc,none",0.35857615130314446
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,arc_ru,"acc_norm,none",0.3242087254063302
MergeBench/Llama-3.2-3B-Instruct_math,multilingual,hellaswag_ru,"acc_norm,none",0.464732528041415
MergeBench/Llama-3.2-3B-Instruct_math,instruction_following,ifeval,"inst_level_loose_acc,none",0.7122302158273381
MergeBench/Llama-3.2-3B-Instruct_math,coding,mbpp_plus,"pass_at_1,none",0.6243386243386243
MergeBench/Llama-3.2-3B-Instruct_math,coding,humaneval_plus,"pass@1,create_test",0.054878048780487805
MergeBench/Llama-3.2-3B-Instruct_math,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.4629289930701372
MergeBench/Llama-3.2-3B-Instruct_math,safety,toxigen,"acc_norm,none",0.8180851063829787
MergeBench/Llama-3.2-3B-Instruct_math,safety,winogender,"winogender_all:acc,none",0.6125
MergeBench/Llama-3.2-3B-Instruct_coding,math,gsm8k_cot,"exact_match,flexible-extract",0.6527672479150872
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,m_mmlu_fr,"acc,none",0.45359407226338705
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,arc_fr,"acc_norm,none",0.378956372968349
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,hellaswag_fr,"acc_norm,none",0.5461554936817306
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,m_mmlu_es,"acc,none",0.45260236988150593
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,arc_es,"acc_norm,none",0.37435897435897436
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,hellaswag_es,"acc_norm,none",0.5614465542991253
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,m_mmlu_de,"acc,none",0.3879167295217982
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,arc_de,"acc_norm,none",0.3438836612489307
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,hellaswag_de,"acc_norm,none",0.5085397096498719
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,m_mmlu_ru,"acc,none",0.4101637579764742
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,arc_ru,"acc_norm,none",0.33704020530367834
MergeBench/Llama-3.2-3B-Instruct_coding,multilingual,hellaswag_ru,"acc_norm,none",0.4783218291630716
MergeBench/Llama-3.2-3B-Instruct_coding,instruction_following,ifeval,"inst_level_loose_acc,none",0.6666666666666666
MergeBench/Llama-3.2-3B-Instruct_coding,coding,mbpp_plus,"pass_at_1,none",0.46296296296296297
MergeBench/Llama-3.2-3B-Instruct_coding,coding,humaneval_plus,"pass@1,create_test",0.1524390243902439
MergeBench/Llama-3.2-3B-Instruct_coding,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.4865539303518384
MergeBench/Llama-3.2-3B-Instruct_coding,safety,toxigen,"acc_norm,none",0.8329787234042553
MergeBench/Llama-3.2-3B-Instruct_coding,safety,winogender,"winogender_all:acc,none",0.5805555555555556
MergeBench/Llama-3.2-3B-Instruct_multilingual,math,gsm8k_cot,"exact_match,flexible-extract",0.7028051554207733
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,m_mmlu_fr,"acc,none",0.48300359025284545
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,arc_fr,"acc_norm,none",0.3900769888793841
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,hellaswag_fr,"acc_norm,none",0.5389805097451275
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,m_mmlu_es,"acc,none",0.5016499175041248
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,arc_es,"acc_norm,none",0.37606837606837606
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,hellaswag_es,"acc_norm,none",0.5485385107744826
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,m_mmlu_de,"acc,none",0.4332478503545029
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,arc_de,"acc_norm,none",0.3473053892215569
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,hellaswag_de,"acc_norm,none",0.5029888983774552
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,m_mmlu_ru,"acc,none",0.43884062427923426
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,arc_ru,"acc_norm,none",0.339606501283148
MergeBench/Llama-3.2-3B-Instruct_multilingual,multilingual,hellaswag_ru,"acc_norm,none",0.46484037963761865
MergeBench/Llama-3.2-3B-Instruct_multilingual,instruction_following,ifeval,"inst_level_loose_acc,none",0.7709832134292566
MergeBench/Llama-3.2-3B-Instruct_multilingual,coding,mbpp_plus,"pass_at_1,none",0.6587301587301587
MergeBench/Llama-3.2-3B-Instruct_multilingual,coding,humaneval_plus,"pass@1,create_test",0.16463414634146342
MergeBench/Llama-3.2-3B-Instruct_multilingual,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5061774384440753
MergeBench/Llama-3.2-3B-Instruct_multilingual,safety,toxigen,"acc_norm,none",0.8159574468085107
MergeBench/Llama-3.2-3B-Instruct_multilingual,safety,winogender,"winogender_all:acc,none",0.6097222222222223
MergeBench/Llama-3.2-3B-Instruct_safety,math,gsm8k_cot,"exact_match,flexible-extract",0.4040940106141016
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,m_mmlu_fr,"acc,none",0.37040714995034757
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,arc_fr,"acc_norm,none",0.34901625320787
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,hellaswag_fr,"acc_norm,none",0.5189548083101306
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,m_mmlu_es,"acc,none",0.2761361931903405
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,arc_es,"acc_norm,none",0.3299145299145299
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,hellaswag_es,"acc_norm,none",0.53147002346917
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,m_mmlu_de,"acc,none",0.26361442148136977
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,arc_de,"acc_norm,none",0.3361847733105218
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,hellaswag_de,"acc_norm,none",0.48665670367207514
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,m_mmlu_ru,"acc,none",0.3738756054432229
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,arc_ru,"acc_norm,none",0.31565440547476475
MergeBench/Llama-3.2-3B-Instruct_safety,multilingual,hellaswag_ru,"acc_norm,none",0.45588869715271785
MergeBench/Llama-3.2-3B-Instruct_safety,instruction_following,ifeval,"inst_level_loose_acc,none",0.6690647482014388
MergeBench/Llama-3.2-3B-Instruct_safety,coding,mbpp_plus,"pass_at_1,none",0.6322751322751323
MergeBench/Llama-3.2-3B-Instruct_safety,coding,humaneval_plus,"pass@1,create_test",0.3719512195121951
MergeBench/Llama-3.2-3B-Instruct_safety,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.45357264460034497
MergeBench/Llama-3.2-3B-Instruct_safety,safety,toxigen,"acc_norm,none",0.8223404255319149
MergeBench/Llama-3.2-3B-Instruct_safety,safety,winogender,"winogender_all:acc,none",0.5986111111111111

model,task_category,task_name,metric_name,score
meta-llama/Llama-3.1-8B-Instruct,math,gsm8k_cot,"exact_match,flexible-extract",0.7952994692949203
meta-llama/Llama-3.1-8B-Instruct,multilingual,m_mmlu_fr,"acc,none",0.48995493086853564
meta-llama/Llama-3.1-8B-Instruct,multilingual,arc_fr,"acc_norm,none",0.4439692044482464
meta-llama/Llama-3.1-8B-Instruct,multilingual,hellaswag_fr,"acc_norm,none",0.603448275862069
meta-llama/Llama-3.1-8B-Instruct,multilingual,m_mmlu_es,"acc,none",0.5497975101244937
meta-llama/Llama-3.1-8B-Instruct,multilingual,arc_es,"acc_norm,none",0.4478632478632479
meta-llama/Llama-3.1-8B-Instruct,multilingual,hellaswag_es,"acc_norm,none",0.6104117772562406
meta-llama/Llama-3.1-8B-Instruct,multilingual,m_mmlu_de,"acc,none",0.489892894855936
meta-llama/Llama-3.1-8B-Instruct,multilingual,arc_de,"acc_norm,none",0.43028229255774164
meta-llama/Llama-3.1-8B-Instruct,multilingual,hellaswag_de,"acc_norm,none",0.5778181041844578
meta-llama/Llama-3.1-8B-Instruct,multilingual,m_mmlu_ru,"acc,none",0.4073960175290228
meta-llama/Llama-3.1-8B-Instruct,multilingual,arc_ru,"acc_norm,none",0.39692044482463645
meta-llama/Llama-3.1-8B-Instruct,multilingual,hellaswag_ru,"acc_norm,none",0.5545729076790337
meta-llama/Llama-3.1-8B-Instruct,instruction_following,ifeval,"inst_level_loose_acc,none",0.854916067146283
meta-llama/Llama-3.1-8B-Instruct,coding,mbpp_plus,"pass_at_1,none",0.0
meta-llama/Llama-3.1-8B-Instruct,coding,humaneval_plus,"pass@1,create_test",0.006097560975609756
meta-llama/Llama-3.1-8B-Instruct,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5511422642920761
meta-llama/Llama-3.1-8B-Instruct,safety,toxigen,"acc_norm,none",0.8425531914893617
meta-llama/Llama-3.1-8B-Instruct,safety,winogender,"winogender_all:acc,none",0.6944444444444444
MergeBench/Llama-3.1-8B-Instruct_instruction,math,gsm8k_cot,"exact_match,flexible-extract",0.01819560272934041
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,m_mmlu_fr,"acc,none",0.44427469253685736
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,arc_fr,"acc_norm,none",0.43199315654405473
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,hellaswag_fr,"acc_norm,none",0.5691796958663525
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,m_mmlu_es,"acc,none",0.4976001199940003
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,arc_es,"acc_norm,none",0.4128205128205128
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,hellaswag_es,"acc_norm,none",0.5761681245999574
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,m_mmlu_de,"acc,none",0.440187056871323
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,arc_de,"acc_norm,none",0.40034217279726264
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,hellaswag_de,"acc_norm,none",0.5392826643894107
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,m_mmlu_ru,"acc,none",0.38448527715845315
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,arc_ru,"acc_norm,none",0.3857998289136014
MergeBench/Llama-3.1-8B-Instruct_instruction,multilingual,hellaswag_ru,"acc_norm,none",0.5202761000862812
MergeBench/Llama-3.1-8B-Instruct_instruction,instruction_following,ifeval,"inst_level_loose_acc,none",0.8513189448441247
MergeBench/Llama-3.1-8B-Instruct_instruction,coding,mbpp_plus,"pass_at_1,none",0.0
MergeBench/Llama-3.1-8B-Instruct_instruction,coding,humaneval_plus,"pass@1,create_test",0.0
MergeBench/Llama-3.1-8B-Instruct_instruction,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5512869974457706
MergeBench/Llama-3.1-8B-Instruct_instruction,safety,toxigen,"acc_norm,none",0.8414893617021276
MergeBench/Llama-3.1-8B-Instruct_instruction,safety,winogender,"winogender_all:acc,none",0.5513888888888889
MergeBench/Llama-3.1-8B-Instruct_math,math,gsm8k_cot,"exact_match,flexible-extract",0.16982562547384383
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,m_mmlu_fr,"acc,none",0.30165762737758767
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,arc_fr,"acc_norm,none",0.3618477331052181
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,hellaswag_fr,"acc_norm,none",0.5786035553651746
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,m_mmlu_es,"acc,none",0.29113544322783863
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,arc_es,"acc_norm,none",0.36666666666666664
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,hellaswag_es,"acc_norm,none",0.5881160657136761
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,m_mmlu_de,"acc,none",0.3239553477145874
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,arc_de,"acc_norm,none",0.3558597091531223
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,hellaswag_de,"acc_norm,none",0.5278608027327071
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,m_mmlu_ru,"acc,none",0.27585146459598675
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,arc_ru,"acc_norm,none",0.31822070145423437
MergeBench/Llama-3.1-8B-Instruct_math,multilingual,hellaswag_ru,"acc_norm,none",0.5064710957722174
MergeBench/Llama-3.1-8B-Instruct_math,instruction_following,ifeval,"inst_level_loose_acc,none",0.35611510791366907
MergeBench/Llama-3.1-8B-Instruct_math,coding,mbpp_plus,"pass_at_1,none",0.1931216931216931
MergeBench/Llama-3.1-8B-Instruct_math,coding,humaneval_plus,"pass@1,create_test",0.0
MergeBench/Llama-3.1-8B-Instruct_math,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.4656134279997085
MergeBench/Llama-3.1-8B-Instruct_math,safety,toxigen,"acc_norm,none",0.5372340425531915
MergeBench/Llama-3.1-8B-Instruct_math,safety,winogender,"winogender_all:acc,none",0.6194444444444445
MergeBench/Llama-3.1-8B-Instruct_coding,math,gsm8k_cot,"exact_match,flexible-extract",0.6254738438210766
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,m_mmlu_fr,"acc,none",0.5412115193644489
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,arc_fr,"acc_norm,none",0.4644995722840034
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,hellaswag_fr,"acc_norm,none",0.6274362818590704
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,m_mmlu_es,"acc,none",0.5407229638518074
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,arc_es,"acc_norm,none",0.42991452991452994
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,hellaswag_es,"acc_norm,none",0.6431619372733092
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,m_mmlu_de,"acc,none",0.5196862271835873
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,arc_de,"acc_norm,none",0.4311377245508982
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,hellaswag_de,"acc_norm,none",0.5939368061485909
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,m_mmlu_ru,"acc,none",0.49488736834012453
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,arc_ru,"acc_norm,none",0.38751069289991447
MergeBench/Llama-3.1-8B-Instruct_coding,multilingual,hellaswag_ru,"acc_norm,none",0.5691328731665228
MergeBench/Llama-3.1-8B-Instruct_coding,instruction_following,ifeval,"inst_level_loose_acc,none",0.6211031175059952
MergeBench/Llama-3.1-8B-Instruct_coding,coding,mbpp_plus,"pass_at_1,none",0.6375661375661376
MergeBench/Llama-3.1-8B-Instruct_coding,coding,humaneval_plus,"pass@1,create_test",0.3902439024390244
MergeBench/Llama-3.1-8B-Instruct_coding,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5135598404471892
MergeBench/Llama-3.1-8B-Instruct_coding,safety,toxigen,"acc_norm,none",0.8202127659574469
MergeBench/Llama-3.1-8B-Instruct_coding,safety,winogender,"winogender_all:acc,none",0.6666666666666666
MergeBench/Llama-3.1-8B-Instruct_multilingual,math,gsm8k_cot,"exact_match,flexible-extract",0.7520849128127369
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,m_mmlu_fr,"acc,none",0.5497670155068367
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,arc_fr,"acc_norm,none",0.47476475620188197
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,hellaswag_fr,"acc_norm,none",0.6501392161062326
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,m_mmlu_es,"acc,none",0.5689215539223039
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,arc_es,"acc_norm,none",0.48547008547008547
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,hellaswag_es,"acc_norm,none",0.6607638148069127
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,m_mmlu_de,"acc,none",0.5457836777794539
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,arc_de,"acc_norm,none",0.4619332763045338
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,hellaswag_de,"acc_norm,none",0.6078138343296328
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,m_mmlu_ru,"acc,none",0.5288690705004997
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,arc_ru,"acc_norm,none",0.42600513259195893
MergeBench/Llama-3.1-8B-Instruct_multilingual,multilingual,hellaswag_ru,"acc_norm,none",0.5806729939603106
MergeBench/Llama-3.1-8B-Instruct_multilingual,instruction_following,ifeval,"inst_level_loose_acc,none",0.7470023980815348
MergeBench/Llama-3.1-8B-Instruct_multilingual,coding,mbpp_plus,"pass_at_1,none",0.05026455026455026
MergeBench/Llama-3.1-8B-Instruct_multilingual,coding,humaneval_plus,"pass@1,create_test",0.036585365853658534
MergeBench/Llama-3.1-8B-Instruct_multilingual,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5296311293438661
MergeBench/Llama-3.1-8B-Instruct_multilingual,safety,toxigen,"acc_norm,none",0.8606382978723405
MergeBench/Llama-3.1-8B-Instruct_multilingual,safety,winogender,"winogender_all:acc,none",0.6902777777777778
MergeBench/Llama-3.1-8B-Instruct_safety,math,gsm8k_cot,"exact_match,flexible-extract",0.0037907505686125853
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,m_mmlu_fr,"acc,none",0.35902528454663507
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,arc_fr,"acc_norm,none",0.3772455089820359
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,hellaswag_fr,"acc_norm,none",0.5744270721781967
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,m_mmlu_es,"acc,none",0.33643317834108294
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,arc_es,"acc_norm,none",0.382051282051282
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,hellaswag_es,"acc_norm,none",0.5955835289097504
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,m_mmlu_de,"acc,none",0.3498265198370795
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,arc_de,"acc_norm,none",0.3669803250641574
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,hellaswag_de,"acc_norm,none",0.5350128095644748
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,m_mmlu_ru,"acc,none",0.3082955331744445
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,arc_ru,"acc_norm,none",0.3575705731394354
MergeBench/Llama-3.1-8B-Instruct_safety,multilingual,hellaswag_ru,"acc_norm,none",0.5203839516824849
MergeBench/Llama-3.1-8B-Instruct_safety,instruction_following,ifeval,"inst_level_loose_acc,none",0.46402877697841727
MergeBench/Llama-3.1-8B-Instruct_safety,coding,mbpp_plus,"pass_at_1,none",0.0
MergeBench/Llama-3.1-8B-Instruct_safety,coding,humaneval_plus,"pass@1,create_test",0.4329268292682927
MergeBench/Llama-3.1-8B-Instruct_safety,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.4713554506600612
MergeBench/Llama-3.1-8B-Instruct_safety,safety,toxigen,"acc_norm,none",0.7170212765957447
MergeBench/Llama-3.1-8B-Instruct_safety,safety,winogender,"winogender_all:acc,none",0.5416666666666666

model,task_category,task_name,metric_name,score
google/gemma-2-9b-it,math,gsm8k_cot,"exact_match,flexible-extract",0.7323730098559514
google/gemma-2-9b-it,multilingual,m_mmlu_fr,"acc,none",0.26193568100221526
google/gemma-2-9b-it,multilingual,arc_fr,"acc_norm,none",0.46706586826347307
google/gemma-2-9b-it,multilingual,hellaswag_fr,"acc_norm,none",0.622295994859713
google/gemma-2-9b-it,multilingual,m_mmlu_es,"acc,none",0.251912404379781
google/gemma-2-9b-it,multilingual,arc_es,"acc_norm,none",0.45897435897435895
google/gemma-2-9b-it,multilingual,hellaswag_es,"acc_norm,none",0.6319607424791978
google/gemma-2-9b-it,multilingual,m_mmlu_de,"acc,none",0.2700256448936491
google/gemma-2-9b-it,multilingual,arc_de,"acc_norm,none",0.4568006843455945
google/gemma-2-9b-it,multilingual,hellaswag_de,"acc_norm,none",0.5950042698548249
google/gemma-2-9b-it,multilingual,m_mmlu_ru,"acc,none",0.2701622203428923
google/gemma-2-9b-it,multilingual,arc_ru,"acc_norm,none",0.446535500427716
google/gemma-2-9b-it,multilingual,hellaswag_ru,"acc_norm,none",0.5776531492666092
google/gemma-2-9b-it,instruction_following,ifeval,"inst_level_loose_acc,none",0.8177458033573142
google/gemma-2-9b-it,coding,mbpp_plus,"pass_at_1,none",0.0
google/gemma-2-9b-it,coding,humaneval_plus,"pass@1,create_test",0.0
google/gemma-2-9b-it,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.610621994844606
google/gemma-2-9b-it,safety,toxigen,"acc_norm,none",0.7797872340425532
google/gemma-2-9b-it,safety,winogender,"winogender_all:acc,none",0.75
MergeBench/gemma-2-9b-it_instruction,math,gsm8k_cot,"exact_match,flexible-extract",0.7293404094010614
MergeBench/gemma-2-9b-it_instruction,multilingual,m_mmlu_fr,"acc,none",0.41020548468413415
MergeBench/gemma-2-9b-it_instruction,multilingual,arc_fr,"acc_norm,none",0.4388366124893071
MergeBench/gemma-2-9b-it_instruction,multilingual,hellaswag_fr,"acc_norm,none",0.5678946241165131
MergeBench/gemma-2-9b-it_instruction,multilingual,m_mmlu_es,"acc,none",0.5213739313034348
MergeBench/gemma-2-9b-it_instruction,multilingual,arc_es,"acc_norm,none",0.41794871794871796
MergeBench/gemma-2-9b-it_instruction,multilingual,hellaswag_es,"acc_norm,none",0.5805419244719436
MergeBench/gemma-2-9b-it_instruction,multilingual,m_mmlu_de,"acc,none",0.5216473072861668
MergeBench/gemma-2-9b-it_instruction,multilingual,arc_de,"acc_norm,none",0.42429426860564584
MergeBench/gemma-2-9b-it_instruction,multilingual,hellaswag_de,"acc_norm,none",0.5323441502988898
MergeBench/gemma-2-9b-it_instruction,multilingual,m_mmlu_ru,"acc,none",0.48865995233335896
MergeBench/gemma-2-9b-it_instruction,multilingual,arc_ru,"acc_norm,none",0.4020530367835757
MergeBench/gemma-2-9b-it_instruction,multilingual,hellaswag_ru,"acc_norm,none",0.5354831751509922
MergeBench/gemma-2-9b-it_instruction,instruction_following,ifeval,"inst_level_loose_acc,none",0.8633093525179856
MergeBench/gemma-2-9b-it_instruction,coding,mbpp_plus,"pass_at_1,none",0.037037037037037035
MergeBench/gemma-2-9b-it_instruction,coding,humaneval_plus,"pass@1,create_test",0.07317073170731707
MergeBench/gemma-2-9b-it_instruction,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.6193285519666009
MergeBench/gemma-2-9b-it_instruction,safety,toxigen,"acc_norm,none",0.7957446808510639
MergeBench/gemma-2-9b-it_instruction,safety,winogender,"winogender_all:acc,none",0.7138888888888889
MergeBench/gemma-2-9b-it_math,math,gsm8k_cot,"exact_match,flexible-extract",0.4116755117513268
MergeBench/gemma-2-9b-it_math,multilingual,m_mmlu_fr,"acc,none",0.2552898938201818
MergeBench/gemma-2-9b-it_math,multilingual,arc_fr,"acc_norm,none",0.369546621043627
MergeBench/gemma-2-9b-it_math,multilingual,hellaswag_fr,"acc_norm,none",0.5807453416149069
MergeBench/gemma-2-9b-it_math,multilingual,m_mmlu_es,"acc,none",0.23173841307934603
MergeBench/gemma-2-9b-it_math,multilingual,arc_es,"acc_norm,none",0.382051282051282
MergeBench/gemma-2-9b-it_math,multilingual,hellaswag_es,"acc_norm,none",0.5948367825901429
MergeBench/gemma-2-9b-it_math,multilingual,m_mmlu_de,"acc,none",0.23698898778096245
MergeBench/gemma-2-9b-it_math,multilingual,arc_de,"acc_norm,none",0.3481608212147134
MergeBench/gemma-2-9b-it_math,multilingual,hellaswag_de,"acc_norm,none",0.5361870196413322
MergeBench/gemma-2-9b-it_math,multilingual,m_mmlu_ru,"acc,none",0.2461751364649804
MergeBench/gemma-2-9b-it_math,multilingual,arc_ru,"acc_norm,none",0.33361847733105215
MergeBench/gemma-2-9b-it_math,multilingual,hellaswag_ru,"acc_norm,none",0.5064710957722174
MergeBench/gemma-2-9b-it_math,instruction_following,ifeval,"inst_level_loose_acc,none",0.28776978417266186
MergeBench/gemma-2-9b-it_math,coding,mbpp_plus,"pass_at_1,none",0.3915343915343915
MergeBench/gemma-2-9b-it_math,coding,humaneval_plus,"pass@1,create_test",0.0
MergeBench/gemma-2-9b-it_math,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.48531575864355786
MergeBench/gemma-2-9b-it_math,safety,toxigen,"acc_norm,none",0.451063829787234
MergeBench/gemma-2-9b-it_math,safety,winogender,"winogender_all:acc,none",0.5472222222222223
MergeBench/gemma-2-9b-it_coding,math,gsm8k_cot,"exact_match,flexible-extract",0.6595905989385898
MergeBench/gemma-2-9b-it_coding,multilingual,m_mmlu_fr,"acc,none",0.5586280650828813
MergeBench/gemma-2-9b-it_coding,multilingual,arc_fr,"acc_norm,none",0.4773310521813516
MergeBench/gemma-2-9b-it_coding,multilingual,hellaswag_fr,"acc_norm,none",0.6587063611051617
MergeBench/gemma-2-9b-it_coding,multilingual,m_mmlu_es,"acc,none",0.5525723713814309
MergeBench/gemma-2-9b-it_coding,multilingual,arc_es,"acc_norm,none",0.4905982905982906
MergeBench/gemma-2-9b-it_coding,multilingual,hellaswag_es,"acc_norm,none",0.6735651802858972
MergeBench/gemma-2-9b-it_coding,multilingual,m_mmlu_de,"acc,none",0.5319052647458139
MergeBench/gemma-2-9b-it_coding,multilingual,arc_de,"acc_norm,none",0.4576561163387511
MergeBench/gemma-2-9b-it_coding,multilingual,hellaswag_de,"acc_norm,none",0.6194491887275833
MergeBench/gemma-2-9b-it_coding,multilingual,m_mmlu_ru,"acc,none",0.5053432766971631
MergeBench/gemma-2-9b-it_coding,multilingual,arc_ru,"acc_norm,none",0.43028229255774164
MergeBench/gemma-2-9b-it_coding,multilingual,hellaswag_ru,"acc_norm,none",0.6053710094909405
MergeBench/gemma-2-9b-it_coding,instruction_following,ifeval,"inst_level_loose_acc,none",0.6139088729016786
MergeBench/gemma-2-9b-it_coding,coding,mbpp_plus,"pass_at_1,none",0.6507936507936508
MergeBench/gemma-2-9b-it_coding,coding,humaneval_plus,"pass@1,create_test",0.6219512195121951
MergeBench/gemma-2-9b-it_coding,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5534065937841098
MergeBench/gemma-2-9b-it_coding,safety,toxigen,"acc_norm,none",0.75
MergeBench/gemma-2-9b-it_coding,safety,winogender,"winogender_all:acc,none",0.6805555555555556
MergeBench/gemma-2-9b-it_multilingual,math,gsm8k_cot,"exact_match,flexible-extract",0.7687642153146323
MergeBench/gemma-2-9b-it_multilingual,multilingual,m_mmlu_fr,"acc,none",0.5942250401038882
MergeBench/gemma-2-9b-it_multilingual,multilingual,arc_fr,"acc_norm,none",0.5466210436270317
MergeBench/gemma-2-9b-it_multilingual,multilingual,hellaswag_fr,"acc_norm,none",0.6790533304776183
MergeBench/gemma-2-9b-it_multilingual,multilingual,m_mmlu_es,"acc,none",0.603794810259487
MergeBench/gemma-2-9b-it_multilingual,multilingual,arc_es,"acc_norm,none",0.552991452991453
MergeBench/gemma-2-9b-it_multilingual,multilingual,hellaswag_es,"acc_norm,none",0.6955408576914871
MergeBench/gemma-2-9b-it_multilingual,multilingual,m_mmlu_de,"acc,none",0.5620003017046311
MergeBench/gemma-2-9b-it_multilingual,multilingual,arc_de,"acc_norm,none",0.5303678357570573
MergeBench/gemma-2-9b-it_multilingual,multilingual,hellaswag_de,"acc_norm,none",0.646135781383433
MergeBench/gemma-2-9b-it_multilingual,multilingual,m_mmlu_ru,"acc,none",0.5657722764665181
MergeBench/gemma-2-9b-it_multilingual,multilingual,arc_ru,"acc_norm,none",0.5064157399486741
MergeBench/gemma-2-9b-it_multilingual,multilingual,hellaswag_ru,"acc_norm,none",0.624137187230371
MergeBench/gemma-2-9b-it_multilingual,instruction_following,ifeval,"inst_level_loose_acc,none",0.6834532374100719
MergeBench/gemma-2-9b-it_multilingual,coding,mbpp_plus,"pass_at_1,none",0.7275132275132276
MergeBench/gemma-2-9b-it_multilingual,coding,humaneval_plus,"pass@1,create_test",0.45121951219512196
MergeBench/gemma-2-9b-it_multilingual,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.585819039965703
MergeBench/gemma-2-9b-it_multilingual,safety,toxigen,"acc_norm,none",0.7138297872340426
MergeBench/gemma-2-9b-it_multilingual,safety,winogender,"winogender_all:acc,none",0.7513888888888889
MergeBench/gemma-2-9b-it_safety,math,gsm8k_cot,"exact_match,flexible-extract",0.5837755875663382
MergeBench/gemma-2-9b-it_safety,multilingual,m_mmlu_fr,"acc,none",0.4270109235352532
MergeBench/gemma-2-9b-it_safety,multilingual,arc_fr,"acc_norm,none",0.3917878528656972
MergeBench/gemma-2-9b-it_safety,multilingual,hellaswag_fr,"acc_norm,none",0.6002355964874706
MergeBench/gemma-2-9b-it_safety,multilingual,m_mmlu_es,"acc,none",0.3559322033898305
MergeBench/gemma-2-9b-it_safety,multilingual,arc_es,"acc_norm,none",0.3923076923076923
MergeBench/gemma-2-9b-it_safety,multilingual,hellaswag_es,"acc_norm,none",0.6067847237038617
MergeBench/gemma-2-9b-it_safety,multilingual,m_mmlu_de,"acc,none",0.32101372756071805
MergeBench/gemma-2-9b-it_safety,multilingual,arc_de,"acc_norm,none",0.369546621043627
MergeBench/gemma-2-9b-it_safety,multilingual,hellaswag_de,"acc_norm,none",0.5494235695986337
MergeBench/gemma-2-9b-it_safety,multilingual,m_mmlu_ru,"acc,none",0.351656800184516
MergeBench/gemma-2-9b-it_safety,multilingual,arc_ru,"acc_norm,none",0.3567151411462789
MergeBench/gemma-2-9b-it_safety,multilingual,hellaswag_ru,"acc_norm,none",0.53958153580673
MergeBench/gemma-2-9b-it_safety,instruction_following,ifeval,"inst_level_loose_acc,none",0.42206235011990406
MergeBench/gemma-2-9b-it_safety,coding,mbpp_plus,"pass_at_1,none",0.6613756613756614
MergeBench/gemma-2-9b-it_safety,coding,humaneval_plus,"pass@1,create_test",0.006097560975609756
MergeBench/gemma-2-9b-it_safety,safety,truthfulqa,"truthfulqa_mc2:acc,none",0.5119508972188703
MergeBench/gemma-2-9b-it_safety,safety,toxigen,"acc_norm,none",0.44148936170212766
MergeBench/gemma-2-9b-it_safety,safety,winogender,"winogender_all:acc,none",0.6222222222222222
